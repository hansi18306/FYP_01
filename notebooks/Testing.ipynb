{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoRAf0imXSvE",
        "outputId": "4930d99e-0d48-4c35-fef4-eba9dd898be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "DM7cd8TpXns0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary resources from NLTK\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmlfK59jX7dJ",
        "outputId": "f2b88fa8-20a4-4bc2-d5fb-cbdceec6ccef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model and tokenizer\n",
        "model = load_model('/content/drive/MyDrive/FYP_New/Code/Model/cyberbullying_model_01.keras')\n",
        "# Assuming you have saved the tokenizer object as 'tokenizer.pickle'\n",
        "import pickle\n",
        "with open('/content/drive/MyDrive/FYP_New/Code/Model/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "metadata": {
        "id": "sHqReTJ0XZmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slang dictionary for preprocessing\n",
        "slang_dict = {\n",
        "    'lmao': 'laughing my ass off',\n",
        "    'k': 'okay',\n",
        "    'y': 'why',\n",
        "    'andme': 'and me',\n",
        "    'ftsomething': 'face time something',\n",
        "    'fkcn': 'fucking',\n",
        "    '1 st': 'first',\n",
        "    'init': 'is not it',\n",
        "    'comp': 'compensation',\n",
        "    'arr': 'arrive',\n",
        "    'studs': 'students',\n",
        "    'tho': 'though',\n",
        "    'irl': 'in real life',\n",
        "    'iykyk': 'if you know, you know',\n",
        "    'fr': 'for real',\n",
        "    'brb': 'be right back',\n",
        "    'idk': 'i do not know',\n",
        "    'imo': 'in my opinion',\n",
        "    'omg': 'oh my god',\n",
        "    'btw': 'by the way',\n",
        "    'ttyl': 'talk to you later',\n",
        "    'smh': 'shaking my head',\n",
        "    'tbh': 'to be honest',\n",
        "    'nvm': 'never mind',\n",
        "    'gtg': 'got to go',\n",
        "    'dm': 'direct message',\n",
        "    'rn': 'right now',\n",
        "    'np': 'no problem',\n",
        "    'lol': 'laughing out loud',\n",
        "    'pls': 'please',\n",
        "    'omw': 'on my way',\n",
        "    'fyi': 'for your information',\n",
        "    'b4': 'before'\n",
        "}"
      ],
      "metadata": {
        "id": "-H54cJ93X_s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to replace slang using slang_dict\n",
        "def replace_slang(text):\n",
        "    for word, replacement in slang_dict.items():\n",
        "        text = re.sub(r'\\b' + re.escape(word) + r'\\b', replacement, text)\n",
        "    return text\n",
        "\n",
        "# Function for full text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Replace slang\n",
        "    text = replace_slang(text)\n",
        "    # Remove numbers and special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize text\n",
        "    tokens = text.split()\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatize the tokens\n",
        "    # tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    # Join tokens back to a single string\n",
        "    clean_text = ' '.join(tokens)\n",
        "    # Function to remove words with less than 3 characters (except 'I' and 'a')\n",
        "    clean_text = remove_short_words(clean_text)\n",
        "    return clean_text\n",
        "\n",
        "def remove_short_words(text):\n",
        "    return ' '.join([word for word in text.split() if len(word) > 2 or word in ['i', 'a']])\n",
        "\n",
        "# Define the predict_cyberbullying_type function\n",
        "def predict_cyberbullying_type(text):\n",
        "    # Preprocess the input text\n",
        "    text_clean = preprocess_text(text)\n",
        "\n",
        "    # Convert the text to a sequence of numerical tokens\n",
        "    text_seq = tokenizer.texts_to_sequences([text_clean])\n",
        "\n",
        "    # Pad the sequence to match the input length of the model\n",
        "    text_pad = pad_sequences(text_seq, maxlen=300, padding='post', truncating='post')\n",
        "\n",
        "    # Make a prediction using the model\n",
        "    prediction = model.predict(text_pad)\n",
        "\n",
        "    # Get the index of the predicted class\n",
        "    predicted_class = np.argmax(prediction)\n",
        "\n",
        "    # Map the predicted class index to the corresponding cyberbullying type\n",
        "    cyberbullying_types = ['religion', 'age', 'gender', 'ethnicity', 'not_cyberbullying']\n",
        "    predicted_type = cyberbullying_types[predicted_class]\n",
        "\n",
        "    return predicted_type"
      ],
      "metadata": {
        "id": "JgQkF-_wYDIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"you're useless\"\n",
        "predicted_type = predict_cyberbullying_type(new_text)\n",
        "print(\"Predicted Cyberbullying Type:\", predicted_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGK8tTBWYLRd",
        "outputId": "91b68944-c725-4a5d-e2d4-8d319a6005b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Predicted Cyberbullying Type: not_cyberbullying\n"
          ]
        }
      ]
    }
  ]
}